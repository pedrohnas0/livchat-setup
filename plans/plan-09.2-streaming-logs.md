# PLAN-09.2: Real-Time Streaming Logs for Remote Exec Jobs

## ðŸ“‹ Contexto

**ReferÃªncia**: PLAN-09.1 implementou job system para remote-bash, mas com **limitaÃ§Ã£o crÃ­tica**:
- âŒ Jobs nÃ£o mostram logs em tempo real durante execuÃ§Ã£o
- âŒ `execute_remote_command()` usa `conn.run()` que Ã© **bloqueante**
- âŒ Logs aparecem apenas NO FINAL da execuÃ§Ã£o

**Status atual**:
- âœ… PLAN-09.1 completo (job decision logic funcionando)
- âš ï¸ Mas sem streaming de output

**Problema identificado**:
```python
# Atual (BLOQUEANTE - sem logs progressivos):
result = await conn.run(command)  # Aguarda atÃ© terminar
# SÃ³ entÃ£o retorna stdout/stderr completos
```

**O que precisa ser**:
```python
# Desejado (STREAMING - logs linha por linha):
async with conn.create_process(command) as proc:
    while True:
        line = await proc.stdout.readline()
        logger.info(f"[stdout] {line}")  # â† Capturado por JobLogManager!
```

---

## ðŸŽ¯ Objetivo

Implementar **streaming real-time de logs** para remote exec jobs:
- âœ… Logs aparecem **linha por linha** durante execuÃ§Ã£o
- âœ… JobLogManager captura automaticamente (jÃ¡ existe!)
- âœ… User monitora progresso com `get-job-status`
- âœ… Suporta comandos de atÃ© 300s
- âœ… SSH mantido aberto durante execuÃ§Ã£o

---

## ðŸ” Descobertas da InvestigaÃ§Ã£o

### âœ… JobLogManager jÃ¡ existe e Ã© robusto!

```python
# src/job_log_manager.py
class JobLogManager:
    MONITORED_MODULES = [
        'src.orchestrator',  # â† JÃ¡ monitora!
        'src.server_setup',
        'src.ansible_executor',
        ...
    ]

    # Features:
    # - Auto-capture via Python logging handlers
    # - Grava em arquivo incrementalmente
    # - MantÃ©m Ãºltimos 100 logs em memÃ³ria
    # - Thread-safe
```

**ImplicaÃ§Ã£o**: Se usarmos `logger.info()` no orchestrator, **logs sÃ£o capturados automaticamente**! ðŸŽ‰

### âœ… asyncssh suporta streaming com create_process()

```python
# MÃ©todo atual (bloqueante):
result = await conn.run(cmd)  # Retorna sÃ³ no fim

# MÃ©todo streaming (linha por linha):
async with conn.create_process(cmd) as proc:
    while True:
        line = await proc.stdout.readline()
        if not line: break
        logger.info(f"[stdout] {line.decode()}")  # â† Auto-capturado!
```

### âœ… Job system jÃ¡ persiste logs incrementalmente

```python
# JobLogManager jÃ¡ faz:
file_handler = RotatingFileHandler(f"{job_id}.log")
# Cada logger.info() Ã© gravado imediatamente no arquivo!
```

---

## ðŸ“Š Escopo Definitivo

### 1. Criar novo mÃ©todo streaming no orchestrator

**Arquivo**: `src/orchestrator/core.py`

```python
async def execute_remote_command_streaming(
    self,
    server_name: str,
    command: str,
    timeout: int = 30,
    working_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    Execute remote command with real-time stdout/stderr streaming

    Logs cada linha de output via logger.info() para captura automÃ¡tica
    pelo JobLogManager. Ideal para jobs de longa duraÃ§Ã£o.

    Returns:
        {
            "stdout": str,  # Output completo
            "stderr": str,  # Errors completos
            "exit_code": int,
            "success": bool
        }
    """
    # ValidaÃ§Ãµes (mesmas do execute_remote_command)
    # ...

    logger.info(f"ðŸ”Œ Connecting to {server_name} ({server_ip}) via SSH...")

    async with asyncssh.connect(...) as conn:
        logger.info(f"â–¶ï¸  Executing: {command}")

        # Streaming com create_process
        async with conn.create_process(full_command) as process:
            stdout_lines = []
            stderr_lines = []

            async def stream_stdout():
                while True:
                    line = await process.stdout.readline()
                    if not line:
                        break
                    decoded = line.decode('utf-8', errors='replace').rstrip()
                    stdout_lines.append(decoded)
                    logger.info(f"ðŸ“¤ {decoded}")  # â† JobLogManager captura!

            async def stream_stderr():
                while True:
                    line = await process.stderr.readline()
                    if not line:
                        break
                    decoded = line.decode('utf-8', errors='replace').rstrip()
                    stderr_lines.append(decoded)
                    logger.warning(f"âš ï¸  {decoded}")  # â† TambÃ©m capturado!

            # Roda ambos em paralelo com timeout
            try:
                await asyncio.wait_for(
                    asyncio.gather(stream_stdout(), stream_stderr()),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.error(f"â±ï¸  Command timed out after {timeout}s")
                process.kill()
                raise

            exit_code = process.returncode
            success = exit_code == 0

            logger.info(f"{'âœ…' if success else 'âŒ'} Command finished: exit_code={exit_code}")

            return {
                "stdout": "\n".join(stdout_lines),
                "stderr": "\n".join(stderr_lines),
                "exit_code": exit_code,
                "success": success
            }
```

**Tamanho estimado**: ~80 linhas (similar ao mÃ©todo atual)

### 2. Atualizar remote_exec executor para usar streaming

**Arquivo**: `src/job_executors/remote_exec_executor.py`

```python
async def execute_remote_exec(job: Job, orchestrator: Orchestrator) -> Dict[str, Any]:
    """Execute remote command with real-time log streaming"""

    server_name = job.params["server_name"]
    command = job.params["command"]

    # Step 1: PreparaÃ§Ã£o
    job.advance_step(1, 2, f"Connecting to {server_name} via SSH")

    try:
        # Usa mÃ©todo STREAMING ao invÃ©s do bloqueante
        result = await orchestrator.execute_remote_command_streaming(
            server_name=server_name,
            command=command,
            timeout=job.params.get("timeout", 30),
            working_dir=job.params.get("working_dir")
        )

        # Logs jÃ¡ foram capturados automaticamente durante streaming!
        # JobLogManager jÃ¡ salvou tudo no arquivo .log

        # Step 2: FinalizaÃ§Ã£o
        success = result["success"]
        exit_code = result["exit_code"]

        if success:
            job.advance_step(2, 2, f"Command completed: exit_code={exit_code}")
        else:
            job.advance_step(2, 2, f"Command failed: exit_code={exit_code}")

        return result

    except Exception as e:
        logger.error(f"Remote exec failed: {e}", exc_info=True)
        job.update_progress(100, f"Failed: {str(e)}")
        raise
```

**MudanÃ§as**:
- âœ… Apenas troca `execute_remote_command` â†’ `execute_remote_command_streaming`
- âœ… Remove `job.add_log()` - JobLogManager cuida disso!
- âœ… Mais simples e limpo

### 3. Manter execute_remote_command original (sync path)

**IMPORTANTE**: NÃƒO substituir o mÃ©todo original!
- âœ… `execute_remote_command()` â†’ Sync path (200 OK)
- âœ… `execute_remote_command_streaming()` â†’ Job path (202 Accepted)

**Por que?**
- Sync path precisa ser **rÃ¡pido** (< 60s)
- Streaming adiciona overhead mÃ­nimo de logging
- UsuÃ¡rio sync nÃ£o precisa de logs detalhados

---

## ðŸ§ª EstratÃ©gia de Testes TDD

### Test 1: Unit test para streaming method (RED â†’ GREEN)

**Arquivo**: `tests/unit/test_remote_exec_streaming.py`

```python
@pytest.mark.asyncio
async def test_execute_remote_command_streaming_logs_output():
    """Should log each line of stdout/stderr in real-time"""

    # Setup
    orchestrator = create_test_orchestrator()

    # Mock asyncssh connection
    with patch('asyncssh.connect') as mock_connect:
        mock_proc = AsyncMock()

        # Simula output linha por linha
        mock_proc.stdout.readline = AsyncMock(side_effect=[
            b"Line 1\n",
            b"Line 2\n",
            b"Line 3\n",
            b""  # EOF
        ])
        mock_proc.stderr.readline = AsyncMock(side_effect=[b""])
        mock_proc.returncode = 0

        mock_conn = AsyncMock()
        mock_conn.create_process = AsyncMock(return_value=mock_proc)
        mock_connect.return_value.__aenter__.return_value = mock_conn

        # Capture logs
        with capture_logs('src.orchestrator') as logs:
            result = await orchestrator.execute_remote_command_streaming(
                server_name="test",
                command="echo test"
            )

        # Assert
        assert "ðŸ“¤ Line 1" in logs
        assert "ðŸ“¤ Line 2" in logs
        assert "ðŸ“¤ Line 3" in logs
        assert result["stdout"] == "Line 1\nLine 2\nLine 3"
```

### Test 2: Integration test com JobLogManager

```python
@pytest.mark.asyncio
async def test_job_executor_saves_logs_to_file():
    """Job executor should save streaming logs to file via JobLogManager"""

    job_manager = JobManager(tmp_path)
    job = await job_manager.create_job(
        job_type="remote_exec",
        params={
            "server_name": "test",
            "command": "for i in 1 2 3; do echo Step $i; done"
        }
    )

    # Execute job
    await job_executor.execute_job(job, orchestrator)

    # Read log file
    log_file = job_manager.log_manager.logs_dir / f"{job.job_id}.log"
    assert log_file.exists()

    content = log_file.read_text()
    assert "ðŸ“¤ Step 1" in content
    assert "ðŸ“¤ Step 2" in content
    assert "ðŸ“¤ Step 3" in content
```

### Test 3: E2E test com servidor real

```typescript
// mcp-server/tests/e2e/test_mcp_e2e_workflow.ts

test('remote-bash job shows real-time logs', async () => {
    // Execute long command via job
    const execResult = await client.request({
        method: 'tools/call',
        params: {
            name: 'remote-bash',
            arguments: {
                server_name: 'e2e-test',
                command: 'for i in {1..5}; do echo "Step $i"; sleep 1; done',
                timeout: 30,
                use_job: true
            }
        }
    });

    const jobId = extractJobId(execResult);

    // Poll job status multiple times
    for (let i = 0; i < 10; i++) {
        await sleep(1000);

        const status = await getJobStatus(jobId, {tail_logs: 20});

        // Should see incremental logs
        expect(status.logs.length).toBeGreaterThan(0);

        if (status.status === 'completed') break;
    }

    // Final check
    const final = await getJobStatus(jobId);
    expect(final.logs).toContainEqual(expect.objectContaining({
        message: expect.stringContaining('ðŸ“¤ Step 1')
    }));
});
```

---

## ðŸ“ Estrutura de Arquivos

```
src/
â”œâ”€â”€ orchestrator/
â”‚   â””â”€â”€ core.py  [MODIFICAR]
â”‚       - Adicionar execute_remote_command_streaming()
â”‚       - Manter execute_remote_command() original
â”‚
â”œâ”€â”€ job_executors/
â”‚   â””â”€â”€ remote_exec_executor.py  [MODIFICAR]
â”‚       - Trocar para execute_remote_command_streaming()
â”‚       - Simplificar logging (JobLogManager cuida)
â”‚
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_remote_exec_streaming.py  [NOVO]
â”‚   â””â”€â”€ test_job_executor.py  [ATUALIZAR]
â”‚
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ test_remote_exec_with_logs.py  [NOVO]
â”‚
â””â”€â”€ e2e/
    â””â”€â”€ (jÃ¡ existe test_mcp_e2e_workflow.ts)
```

---

## âœ… Checklist de ImplementaÃ§Ã£o

### Etapa 1: TDD - Criar testes (RED phase)
- [ ] Test: streaming method loga cada linha
- [ ] Test: JobLogManager captura logs automaticamente
- [ ] Test: Timeout funciona corretamente
- [ ] Test: Stderr Ã© capturado separadamente
- [ ] Rodar testes â†’ Devem FALHAR (RED)

### Etapa 2: Implementar streaming method (GREEN phase)
- [ ] Criar `execute_remote_command_streaming()` em orchestrator
- [ ] Usar `conn.create_process()` para streaming
- [ ] Usar `logger.info()` para cada linha
- [ ] Handle timeout com asyncio.wait_for()
- [ ] Rodar testes â†’ Devem PASSAR (GREEN)

### Etapa 3: Atualizar executor
- [ ] Modificar `execute_remote_exec()` usar mÃ©todo streaming
- [ ] Remover `job.add_log()` manual
- [ ] Simplificar cÃ³digo
- [ ] Rodar testes unitÃ¡rios

### Etapa 4: Testar via terminal
- [ ] Comando rÃ¡pido (sync): `curl` com timeout=30
- [ ] Comando longo (job): `curl` com use_job=true
- [ ] Monitorar job: `get-job-status` mÃºltiplas vezes
- [ ] Validar logs aparecem incrementalmente
- [ ] Verificar arquivo .log no disco

### Etapa 5: Validar com E2E
- [ ] Atualizar test E2E para validar logs
- [ ] Rodar E2E completo: `npm run test:e2e`
- [ ] Validar todos cenÃ¡rios passam

---

## ðŸ“¦ DependÃªncias Novas

**Nenhuma!** âœ…

Tudo jÃ¡ existe:
- âœ… asyncssh (jÃ¡ instalado)
- âœ… JobLogManager (jÃ¡ implementado)
- âœ… Job system (jÃ¡ funcional)

---

## ðŸŽ¯ CritÃ©rios de Sucesso

### User Experience esperada:

```bash
# User executa comando longo:
remote-bash(
    server="prod",
    command="apt-get update && apt-get upgrade -y",
    timeout=180,
    use_job=true
)

# Resposta:
{job_id: "remote_exec-abc123"}

# User monitora (a cada 2s):
get-job-status(job_id="remote_exec-abc123", tail_logs=20)

# Output evolui em tempo real:
[19:10:01] ðŸ”Œ Connecting to prod (1.2.3.4) via SSH...
[19:10:02] â–¶ï¸  Executing: apt-get update && apt-get upgrade -y
[19:10:03] ðŸ“¤ Hit:1 http://deb.debian.org/debian bookworm InRelease
[19:10:04] ðŸ“¤ Get:2 http://security.debian.org bookworm-security InRelease
[19:10:05] ðŸ“¤ Reading package lists...
[19:10:10] ðŸ“¤ Building dependency tree...
...
[19:12:30] âœ… Command finished: exit_code=0
```

### ValidaÃ§Ãµes tÃ©cnicas:

âœ… **Logs aparecem linha por linha** durante execuÃ§Ã£o
âœ… **JobLogManager salva no arquivo** incrementalmente
âœ… **get-job-status retorna logs recentes** de memÃ³ria
âœ… **Timeout funciona** (kill process apÃ³s N segundos)
âœ… **Stderr separado** de stdout
âœ… **Exit code correto** ao final

---

## âš ï¸ ConsideraÃ§Ãµes Importantes

### 1. Performance

**Streaming vs Bloqueante**:
- Streaming: +5-10ms overhead por linha (mÃ­nimo)
- Bloqueante: 0ms overhead (retorna tudo no fim)

**DecisÃ£o**: AceitÃ¡vel para job path (jÃ¡ Ã© longo)

### 2. Limite de output

**Problema**: Comando pode gerar 100MB de output

**SoluÃ§Ã£o atual**: Truncar stdout/stderr em 10KB
```python
# Manter limite em 10KB para evitar memory issues
if len(stdout_lines) > 1000:  # ~10KB
    stdout_lines = stdout_lines[:1000]
    logger.warning("Output truncated: exceeded 10KB limit")
```

### 3. Encoding

**Problema**: Output pode ter encoding variado

**SoluÃ§Ã£o**: `decode('utf-8', errors='replace')`
- Replace caracteres invÃ¡lidos com ï¿½
- Nunca falha

### 4. Backward compatibility

âœ… **Sync path nÃ£o afetado** - continua usando mÃ©todo original
âœ… **Jobs migram automaticamente** - sÃ³ trocar mÃ©todo
âœ… **API nÃ£o muda** - mesmos endpoints

---

## ðŸš€ PrÃ³ximos Passos (PÃ³s-09.2)

ApÃ³s implementar streaming:
1. **PLAN-10**: Add job cancellation (kill SSH process)
2. **PLAN-11**: WebSocket support para live logs (sem polling)
3. **PLAN-12**: Log filtering (por level, regex)

---

## ðŸ“Š Status

ðŸ”µ **READY TO START** - Plano aprovado, aguardando implementaÃ§Ã£o

---

*Plan Version: 1.0.0*
*Created: 2025-10-21*
*Based on: PLAN-09.1 + Investigation findings*
